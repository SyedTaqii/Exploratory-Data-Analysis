{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(df, target_col, features_cols):\n",
    "    X = df[features_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split into Training & Testing Sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using MSE, RMSE, and R2 score\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    print(f'R2 Score: {r2}')\n",
    "    \n",
    "    return model, y_test, y_pred\n",
    "\n",
    "def plot_actual_and_predicted(y_test, y_pred):\n",
    "    # Plotting actual and predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, color='blue', alpha=0.7)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--') \n",
    "    plt.title('Actual vs Predicted Electricity Demand')\n",
    "    plt.xlabel('Actual Demand')\n",
    "    plt.ylabel('Predicted Demand')\n",
    "    plt.show()\n",
    "\n",
    "def residual_analysis(y_test, y_pred):\n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Plotting residuals\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, residuals, color='blue', alpha=0.7)\n",
    "    plt.axhline(y=0, color='red', linestyle='--')  # Horizontal line at 0\n",
    "    plt.title('Residuals vs Predicted Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()\n",
    "\n",
    "def perform_regression_model(df):\n",
    "    timestamp_col = \"timestamp\"\n",
    "    target_col = \"demand\"\n",
    "\n",
    "    feature_cols = [ \"temperature\", \"year\", \"month\", \"day\", \"hour\", \"day_of_week\", \"is_weekend\"]\n",
    "\n",
    "    model, y_test, y_pred = evaluate_model(df, target_col, feature_cols)\n",
    "\n",
    "    plot_actual_and_predicted(y_test, y_pred)\n",
    "\n",
    "    residual_analysis(y_test, y_pred)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "# IQR-based Outlier Detection\n",
    "def detect_outliers_iqr(df):\n",
    "    outliers = {}\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index\n",
    "    return outliers\n",
    "\n",
    "# Z-score-based Outlier Detection\n",
    "def detect_outliers_zscore(df, threshold=3):\n",
    "    outliers = {}\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        z_scores = zscore(df[col])\n",
    "        outliers[col] = df[np.abs(z_scores) > threshold].index\n",
    "    return outliers\n",
    "\n",
    "def plot_data_before_after(df, df_cleaned_iqr, df_cleaned_zscore):\n",
    "    # Plot original data\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.boxplot(data=df)\n",
    "    plt.title('Original Data')\n",
    "\n",
    "    # After modification (cleaned data)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(data=df_cleaned_iqr)\n",
    "    plt.title('After Handling IQR Outliers')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(data=df_cleaned_zscore)\n",
    "    plt.title('After Handling Z-Score Outliers')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def handle_outliers(df, outliers, strategy=\"remove\", cap_value=None):\n",
    "    df_copy = df.copy()\n",
    "    for col, indices in outliers.items():\n",
    "        if strategy == \"remove\":\n",
    "            df_copy = df_copy.drop(index=indices, errors='ignore')\n",
    "        elif strategy == \"cap\":\n",
    "            lower_limit = df_copy[col].quantile(0.05) if cap_value is None else cap_value\n",
    "            upper_limit = df_copy[col].quantile(0.95) if cap_value is None else cap_value\n",
    "            df_copy[col] = np.clip(df_copy[col], lower_limit, upper_limit)\n",
    "        elif strategy == \"transform\":\n",
    "            df_copy[col] = np.log1p(df_copy[col])  # Log transformation\n",
    "    return df_copy\n",
    "\n",
    "def detect_outliers(df):\n",
    "    outliers_iqr = detect_outliers_iqr(df)\n",
    "    outliers_zscore = detect_outliers_zscore(df)\n",
    "\n",
    "    clenaed_df = df_iqr_removed = handle_outliers(df, outliers_iqr, strategy=\"remove\")\n",
    "    clenaed_df = df_zscore_transformed = handle_outliers(df, outliers_zscore, strategy=\"transform\")\n",
    "\n",
    "    plot_data_before_after(df, df_iqr_removed, df_zscore_transformed)\n",
    "    return clenaed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "def perform_stats_analysis(df):\n",
    "    \"\"\"Compute basic statistics (mean, median, std, etc.) for each numerical column.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns  \n",
    "    stats_df = pd.DataFrame(columns=[\"Mean\", \"Median\", \"Standard Deviation\", \"Variance\", \n",
    "                                     \"Skewness\", \"Kurtosis\", \"Min\", \"Max\"])\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        stats_df.loc[col] = {\n",
    "            \"Mean\": df[col].mean(),\n",
    "            \"Median\": df[col].median(),\n",
    "            \"Standard Deviation\": df[col].std(),\n",
    "            \"Variance\": df[col].var(),\n",
    "            \"Skewness\": df[col].skew(),\n",
    "            \"Kurtosis\": stats.kurtosis(df[col], fisher=True),\n",
    "            \"Min\": df[col].min(),\n",
    "            \"Max\": df[col].max()\n",
    "        }\n",
    "    \n",
    "    print(\"\\nStatistical Summary of Numeric Features\")\n",
    "    print(stats_df)\n",
    "\n",
    "def plot_time_series(df):\n",
    "    \"\"\"Plot electricity demand over time with a line plot.\"\"\"\n",
    "\n",
    "    timestamp_col = \"timestamp\"\n",
    "    demand_col = \"demand\"\n",
    "    df.loc[:, timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    df.sort_values(by=timestamp_col)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df[timestamp_col], df[demand_col], label=\"Electricity Demand\", color='blue')\n",
    "    plt.title(\"Electricity Demand Over Time\", fontsize=14)\n",
    "    plt.xlabel(\"Time\", fontsize=12)\n",
    "    plt.ylabel(\"Electricity Demand\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def perform_univariate_analysis(df):\n",
    "    \"\"\"Generate histograms, boxplots, and density plots for numerical features.\"\"\"\n",
    "\n",
    "    df.loc[:, \"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    for column in numeric_cols:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.histplot(df[column], kde=True)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.boxplot(y=df[column])\n",
    "        plt.title(f\"Boxplot of {column}\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.kdeplot(df[column], fill=True)\n",
    "        plt.title(f\"Density Plot of {column}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "def perform_correlation_analysis(df):\n",
    "    \"\"\"Compute and visualize correlation matrix for numerical features.\"\"\"\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "def perform_advance_time_series_analysis(df):\n",
    "    \"\"\"Decompose the time series and perform a stationarity test.\"\"\"\n",
    "\n",
    "    timestamp_col = \"timestamp\"\n",
    "    demand_col = \"demand\"\n",
    "    df.loc[:, timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    df.set_index(timestamp_col, inplace=True)\n",
    "\n",
    "    decomposition = seasonal_decompose(df[demand_col], model='additive', period=365)\n",
    "    decomposition.plot()\n",
    "    plt.show()\n",
    "    perform_stationarity_test(df, timestamp_col, demand_col)\n",
    "\n",
    "def perform_stationarity_test(df, timestamp_col, demand_col):\n",
    "    \"\"\"Conduct Augmented Dickey-Fuller test to check for stationarity.\"\"\"\n",
    "    \n",
    "    result = adfuller(df[demand_col].dropna())\n",
    "    print(\"\\nAugmented Dickey-Fuller Test Results\")\n",
    "    print(f\"ADF Statistic: {result[0]}\")\n",
    "    print(f\"p-value: {result[1]}\")\n",
    "    print(f\"Critical Values: {result[4]}\")\n",
    "    \n",
    "    if result[1] < 0.05:\n",
    "        print(\"The time series is stationary (rejecting null hypothesis).\")\n",
    "    else:\n",
    "        print(\"The time series is non-stationary (fail to reject null hypothesis).\")\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "def perform_eda(df):\n",
    "\n",
    "    perform_stats_analysis(df)\n",
    "    plot_time_series(df)\n",
    "    perform_univariate_analysis(df)\n",
    "    perform_correlation_analysis(df)\n",
    "    perform_advance_time_series_analysis(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def test_mcar(df):\n",
    "    df_miss = df.isnull().astype(int)\n",
    "    chi2, p, _, _ = chi2_contingency(df_miss.corr())\n",
    "    return p \n",
    "\n",
    "def missing_data_consistencies(df):\n",
    "    # Calculate missing counts and percentages\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentage = (missing_counts / len(df)) * 100\n",
    "\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_counts,\n",
    "        'Missing Percentage': missing_percentage\n",
    "    })\n",
    "    \n",
    "    print(\"Missing Value Summary:\")\n",
    "    print(missing_df)\n",
    "\n",
    "    #Determine missing value type\n",
    "    p_missing_value = test_mcar(df)\n",
    "    if p_missing_value > 0.05:\n",
    "        print(\"Missing data is most likely MCAR\")\n",
    "    else:\n",
    "        print(\"Missing data is likely MAR or MNAR\")  \n",
    "\n",
    "    #perform imputation\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype == 'object':  # string type values or other objects\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True) #mode\n",
    "            else: \n",
    "                df[col].fillna(df[col].median(), inplace=True)  #median\n",
    "\n",
    "    print(\"Missing values handled\") \n",
    "\n",
    "    return df \n",
    "\n",
    "def duplicate_and_inconsistencies(df):\n",
    "    df = df.drop_duplicates() #remove duplicates\n",
    "\n",
    "    for col in df.select_dtypes(include=['number']):\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Identify outliers\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        \n",
    "        print(f\"Outliers detected in {col}: {len(outliers)} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_feature(df):\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.loc[:, 'timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "        df.loc[:, 'year'] = df['timestamp'].dt.year\n",
    "        df.loc[:, 'month'] = df['timestamp'].dt.month\n",
    "        df.loc[:, 'day'] = df['timestamp'].dt.day\n",
    "        df.loc[:, 'hour'] = df['timestamp'].dt.hour\n",
    "        df.loc[:, 'day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "        df.loc[:, 'is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "        def get_season(month):\n",
    "            if month in [12, 1, 2]:  \n",
    "                return \"Winter\"\n",
    "            elif month in [3, 4, 5]:  \n",
    "                return \"Spring\"\n",
    "            elif month in [6, 7, 8]:  \n",
    "                return \"Summer\"\n",
    "            else:  \n",
    "                return \"Autumn\"\n",
    "\n",
    "        df.loc[:, 'season'] = df['month'].apply(get_season)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_standardize(df, method=\"normalize\"):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    if method == \"normalize\":\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "def pre_process_data(df):\n",
    "    #data_conversions\n",
    "    df[\"demand\"] = pd.to_numeric(df[\"demand\"], errors='coerce')\n",
    "    df.sort_values(by=\"timestamp\", inplace=True)\n",
    "\n",
    "    df = missing_data_consistencies(df)\n",
    "    df = duplicate_and_inconsistencies(df)\n",
    "    df = engineer_feature(df)\n",
    "    df = normalize_standardize(df, method=\"normalize\") # Change to \"standardize\" if needed\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import process_data\n",
    "import eda\n",
    "import outliers\n",
    "import regression_model\n",
    "\n",
    "# File Paths\n",
    "weather_dir = r\".\\data\\raw\\weather_raw_data\"\n",
    "electricity_dir = r\".\\data\\raw\\electricity_raw_data\"\n",
    "\n",
    "def load_weather_data(folder):\n",
    "    weather_data = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors='coerce', utc=True)\n",
    "            weather_data.append(df)\n",
    "    weather_df = pd.concat(weather_data, ignore_index=True)\n",
    "    weather_df = weather_df.rename(columns={\"date\": \"timestamp\", \"temperature_2m\": \"temperature\"})\n",
    "    weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"], errors='coerce', utc=True)\n",
    "    return weather_df\n",
    "\n",
    "def load_electricity_data(folder):\n",
    "    electricity_data = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(folder, file), 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)[\"response\"][\"data\"]\n",
    "                    electricity_data.extend(data)\n",
    "                except KeyError:\n",
    "                    print(f\"Unexpected error, Skipping file\")\n",
    "    electricity_df = pd.DataFrame(electricity_data)\n",
    "    electricity_df[\"period\"] = pd.to_datetime(electricity_df[\"period\"], errors='coerce')\n",
    "    electricity_df[\"value\"] = pd.to_numeric(electricity_df[\"value\"], errors='coerce')\n",
    "    electricity_df.rename(columns={\"period\": \"timestamp\", \"value\": \"demand\"}, inplace=True)\n",
    "    electricity_df[\"timestamp\"] = pd.to_datetime(electricity_df[\"timestamp\"], errors='coerce', utc=True)\n",
    "    return electricity_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    weather_df = load_weather_data(weather_dir)\n",
    "    electricity_df = load_electricity_data(electricity_dir)\n",
    "\n",
    "    merged_data_df = pd.merge(electricity_df, weather_df, on=\"timestamp\", how=\"inner\")\n",
    "    merged_data_df.sort_values(by=\"timestamp\", inplace=True)\n",
    "    merged_data_df.to_csv(\"merged_data.csv\", index=False)\n",
    "\n",
    "    processed_data_df = process_data.pre_process_data(merged_data_df)\n",
    "    processed_data_df.to_csv(\"processed_data.csv\", index=False)\n",
    "\n",
    "    eda.perform_eda(processed_data_df)\n",
    "\n",
    "    clean_df = outliers.detect_outliers(processed_data_df)\n",
    "    clean_df.to_csv(\"cleaned_data.csv\", index=False)\n",
    "\n",
    "    model, predictions = regression_model.perform_regression_model(clean_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
